{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Computing - Summer 2017\n",
    "# Exercise 4 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4.1. K-means Clustering of couse network data\n",
    "\n",
    "Write a Python program that computes K-means clustering for the course dataset. The input dataset (file: networkinput.csv) contains data from our social networking platform. As always this data has been anonymized. Each line in the file represents one feature vector and is associated with a participant in the social networking platform. The vectors have a name (\"userXYZ\") and the following 4 features:\n",
    "* Number of posts\n",
    "* Number of comments\n",
    "* Number of likes (on both posts and comments)\n",
    "* Number of friends and followers (average)\n",
    "\n",
    "By clustering the participants, you identify particpants with similar activity patterns. This can be helpful for research but also for advertisers and polling firms.\n",
    "\n",
    "Your program should compute K-means clustering for the dataset according to the formula discussed in the lecture ==> Minimizing the objective function:\n",
    "$$\\sum_{k=1}^{K} \\sum_{\\{n|x_n \\in C_k \\}} \\|x_n - \\mu_k\\|^2$$\n",
    "Such that:\n",
    "$K$ is the number of clusters, $x_n$ is the nth point that belongs to the $k$th cluster, and $\\mu_k$ is the centroid (prototype) of the $k$th cluster. (Refer to the lecture for details)\n",
    "The K-means clustering algorithm should proceed as follows:\n",
    "1. The program should start by parsing the dataset \n",
    "2. Assign four random centroids (prototypes).\n",
    "3. Assign data points to the nearest centroid. \n",
    "4. Recompute the centroid values: The new centroid values of the kth centroid are calculated as the average values of the points currently in that centroid.\n",
    "5. Repeat from point 3 till the values of the centroids don't change anymore.\n",
    "\n",
    "<b>The output of your program should be a a list that asigns a cluster ID (0, 1, 2, 3) to every user in the input file. </b>\n",
    "The first argument in that tuple should be the users's name (e.g., \"user111\") and the second argument should be the centroid id to which this user is associated to. e.g ('user111', 3).\n",
    "**Note:** this output value should be the final centroid values that don't change anymore.\n",
    "\n",
    "\n",
    "<b> After the clustering with k=4 is complete, run the code with the following centroid starting points:\n",
    "\n",
    "centroids = {0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]}.\n",
    "\n",
    "Have a look at the result and describe the common properties in each of the four groups</b> (max 5 sentences).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 33 29 25] [ 4 44 12 41] [10 13 44 65] [10 44 48 70]\n",
      "[ 0.44204852  3.01617251  3.54716981  4.85175202] [ 0.  0.  0.  0.] [  0.5   9.5  33.5  48. ] [  7.5  38.   56.   59.5]\n",
      "[ 0.81460674  5.62921348  6.20786517  7.65730337] [ 0.          0.1657754   0.04278075  1.5828877 ] [  2.5    13.125  33.75   29.625] [  7.5  38.   56.   59.5]\n",
      "[ 1.08333333  7.3         8.04166667  8.85      ] [ 0.05371901  0.48760331  0.25619835  2.31818182] [  2.          13.09090909  32.36363636  24.81818182] [  7.5  38.   56.   59.5]\n",
      "[ 1.19387755  7.86734694  8.71428571  9.2244898 ] [ 0.05747126  0.73180077  0.36015326  2.65517241] [  2.46153846  12.61538462  29.38461538  19.38461538] [  5.33333333  29.33333333  55.          55.33333333]\n",
      "[ 1.15053763  7.91397849  8.52688172  9.44086022] [ 0.05681818  0.76893939  0.37121212  2.73106061] [  2.8         12.46666667  29.26666667  16.66666667] [  5.33333333  29.33333333  55.          55.33333333]\n",
      "[ 1.01136364  7.94318182  7.75        9.54545455] [ 0.05681818  0.76893939  0.37121212  2.73106061] [  3.   11.2  27.5  14.4] [  5.33333333  29.33333333  55.          55.33333333]\n",
      "[ 0.94186047  7.8372093   7.27906977  9.51162791] [ 0.05703422  0.76045627  0.36882129  2.69961977] [  2.95652174  10.95652174  26.39130435  13.95652174] [  5.33333333  29.33333333  55.          55.33333333]\n",
      "[ 0.88235294  7.56470588  6.98823529  9.49411765] [ 0.05725191  0.7519084   0.37022901  2.66412214] [  2.96  11.44  25.56  13.76] [  5.33333333  29.33333333  55.          55.33333333]\n",
      "[ 0.84269663  7.34831461  6.71910112  9.52808989] [ 0.05813953  0.72093023  0.36046512  2.54651163] [  2.96  11.44  25.56  13.76] [  5.33333333  29.33333333  55.          55.33333333]\n",
      "{0: [0.84269662921348309, 7.3483146067415728, 6.7191011235955056, 9.5280898876404496], 1: [0.058139534883720929, 0.72093023255813948, 0.36046511627906974, 2.5465116279069768], 2: [2.96, 11.44, 25.559999999999999, 13.76], 3: [5.333333333333333, 29.333333333333332, 55.0, 55.333333333333336]}\n",
      "{0: [0.84269662921348309, 7.3483146067415728, 6.7191011235955056, 9.5280898876404496], 1: [0.058139534883720929, 0.72093023255813948, 0.36046511627906974, 2.5465116279069768], 2: [2.96, 11.44, 25.559999999999999, 13.76], 3: [5.333333333333333, 29.333333333333332, 55.0, 55.333333333333336]}\n",
      "10\n",
      "89\n",
      "258\n",
      "25\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import random \n",
    "import numpy as np \n",
    "from scipy.spatial import distance \n",
    "\n",
    "# TODO: Read the network activity data set into a dictionary (observations_dictionary)\n",
    "def read_data_set():\n",
    "    observations_dictionary = {}\n",
    "    dataFromCSV = csv.reader(open('networkinput.csv'))\n",
    "    for row in dataFromCSV:\n",
    "        key = row[0]\n",
    "        row[1]=float(row[1])\n",
    "        row[2]=float(row[2])\n",
    "        row[3]=float(row[3])\n",
    "        row[4]=float(row[4])\n",
    "        observations_dictionary[key] = row[1:]\n",
    "    return observations_dictionary\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Assign random centroids\n",
    "# Hint: centroid values should be randomly assigned so that for each dimension of the centroid, the random values should fall\n",
    "# between the maximum and the minimum value of that dimension of all the points in the data set.\n",
    "# For example: for the dimension \"Number of Comments\", if the maximum value = 49, and the minimum value = 0, the value assigned to\n",
    "# the dimension \"Murder\" of the centroid should be randomly assigned between 0 and 49\n",
    "def create_random_centroid_values(observations):\n",
    "    # find min and max of observations in each dimension\n",
    "    # create dictionary with four random centroids within the observed space\n",
    "    posts = []\n",
    "    comments = []\n",
    "    likes = []\n",
    "    friends = []\n",
    "    centroid = {}\n",
    "    for value in observations.values():\n",
    "        post_dimension.append(value[0])\n",
    "        comment_dimension.append(value[1])\n",
    "        likes_dimension.append(value[2])\n",
    "        friends_dimension.append(value[3])\n",
    "    #Calculating Minimum Values for all four dimensions\n",
    "    minPosts= min(posts)  \n",
    "    minLikes = min(likes)\n",
    "    minComments= min(comments)\n",
    "    minFriends = min(friends)\n",
    "    \n",
    "    #Calculating Maximum Values for all four dimensions\n",
    "    maxPosts= max(posts)\n",
    "    maxComments = max(comments)\n",
    "    maxLikes = max(likes)\n",
    "    maxFriends = max(friends)\n",
    "    \n",
    "    #Gererating a centroid point by picking a random number between min and max of each dimension\n",
    "    i=0\n",
    "    while i < 4:\n",
    "        randomCentroid = []\n",
    "        randomCentroid.append(random.randint(minPosts,maxPosts))\n",
    "        randomCentroid.append(random.randint(minComments,maxComments))\n",
    "        randomCentroid.append(random.randint(minLikes,maxLikes))\n",
    "        randomCentroid.append(random.randint(minFriends,maxFriends))\n",
    "        centroid[i]= randomCentroid\n",
    "        i+=1\n",
    "    return  centroid   \n",
    "    \n",
    "        \n",
    "    \n",
    "# Assign centroid ID for each of the data points. Each data item is assigned to its closest centroid\n",
    "def update_observation_centroids(observations, centroid, k, observation_centroids = None):\n",
    "    # create dictionary mapping each observation to a centroid index\n",
    "    if observation_centroids == None: \n",
    "        # initial run: For each centroid i of the k centroids, create random values\n",
    "        #create_random_centroid_values(observations, centroid_i)\n",
    "        \n",
    "        observation_centroids = {} \n",
    "        c0 = np.array(centroid[0])\n",
    "        c1 = np.array(centroid[1])\n",
    "        c2 = np.array(centroid[2])\n",
    "        c3 = np.array(centroid[3])\n",
    "        print c0,c1,c2,c3\n",
    "        for point in observations:\n",
    "            x = np.array(observations[point])\n",
    "            dist1 = distance.euclidean(x,c0)\n",
    "            dist2 = distance.euclidean(x,c1)\n",
    "            dist3 = distance.euclidean(x,c2)\n",
    "            dist4 = distance.euclidean(x,c3)\n",
    "\n",
    "            low_dist = dist1\n",
    "            cluster_id=0\n",
    "            \n",
    "            if(low_dist>dist2):\n",
    "                low_dist = dist2\n",
    "                cluster_id=1\n",
    "            if(low_dist>dist3):\n",
    "                low_dist = dist3\n",
    "                cluster_id=2\n",
    "            if(low_dist>dist4):\n",
    "                low_dist=dist4\n",
    "                cluster_id=3\n",
    "            observation_centroids[point]=cluster_id\n",
    "            \n",
    "    return observation_centroids# TODO: return the updated centroids\n",
    "    \n",
    "    # Otherwise\n",
    "    #for key in observations:\n",
    "        # TODO: Each centroid i of the K centroids is the average of the data values previously assigned to that centroid i\n",
    "    \n",
    "    #return # TODO: Return the newly created observation centroids\n",
    "\n",
    "\n",
    "# Create new centroid values for each cluster as the mean of data values for the points in that cluster \n",
    "# Create new centroid values for each cluster as the mean of data values for the points in that cluster \n",
    "def update_centroid_values(observations, observation_centroids, k):\n",
    "    centroid1_list = []\n",
    "    centroid2_list = []\n",
    "    centroid3_list = []\n",
    "    centroid4_list = []\n",
    "    \n",
    "    for item in observation_centroids:\n",
    "        if(observation_centroids[item]==0):\n",
    "            centroid1_list.append(item)\n",
    "        if(observation_centroids[item]==1):\n",
    "            centroid2_list.append(item)\n",
    "        if(observation_centroids[item]==2):\n",
    "            centroid3_list.append(item)\n",
    "        if(observation_centroids[item]==3):\n",
    "            centroid4_list.append(item)\n",
    "    \n",
    "    #calculate new centroid 1\n",
    "    new_centroid = {}\n",
    "    post_dimension = []\n",
    "    comment_dimension = []\n",
    "    likes_dimension = []\n",
    "    friends_dimension = []\n",
    "    for item in centroid1_list:\n",
    "        post_dimension.append(observations[item][0])\n",
    "        comment_dimension.append(observations[item][1])\n",
    "        likes_dimension.append(observations[item][2])\n",
    "        friends_dimension.append(observations[item][3])\n",
    "    l= []\n",
    "    if(len(post_dimension)!=0):\n",
    "        l.append(np.mean(post_dimension))        \n",
    "        l.append(np.mean(comment_dimension))\n",
    "        l.append(np.mean(likes_dimension))\n",
    "        l.append(np.mean(friends_dimension))\n",
    "    else:\n",
    "        l=[0.0,0.0,0.0,0.0]\n",
    "    new_centroid[0] = l\n",
    "    \n",
    "    #calculate new centroid 2\n",
    "    post_dimension = []\n",
    "    comment_dimension = []\n",
    "    likes_dimension = []\n",
    "    friends_dimension = []\n",
    "    for item in centroid2_list:\n",
    "        post_dimension.append(int(observations[item][0]))\n",
    "        comment_dimension.append(int(observations[item][1]))\n",
    "        likes_dimension.append(int(observations[item][2]))\n",
    "        friends_dimension.append(int(observations[item][3]))\n",
    "    l=[]\n",
    "    if(len(post_dimension)!=0):\n",
    "        l.append(np.mean(post_dimension))\n",
    "        l.append(np.mean(comment_dimension))\n",
    "        l.append(np.mean(likes_dimension))\n",
    "        l.append(np.mean(friends_dimension))\n",
    "    else:\n",
    "        l=[0.0,0.0,0.0,0.0]\n",
    "    new_centroid[1] = l\n",
    "    \n",
    "    #calculate new centroid 3\n",
    "    post_dimension = []\n",
    "    comment_dimension = []\n",
    "    likes_dimension = []\n",
    "    friends_dimension = []\n",
    "    for item in centroid3_list:\n",
    "        post_dimension.append(int(observations[item][0]))\n",
    "        comment_dimension.append(int(observations[item][1]))\n",
    "        likes_dimension.append(int(observations[item][2]))\n",
    "        friends_dimension.append(int(observations[item][3]))\n",
    "    l= []\n",
    "    if(len(post_dimension)!=0):\n",
    "        l.append(np.mean(post_dimension))\n",
    "        l.append(np.mean(comment_dimension))\n",
    "        l.append(np.mean(likes_dimension))\n",
    "        l.append(np.mean(friends_dimension))\n",
    "    else:\n",
    "        l=[0.0,0.0,0.0,0.0]\n",
    "    new_centroid[2] = l\n",
    "    \n",
    "    #calculate new centroid 4\n",
    "    post_dimension = []\n",
    "    comment_dimension = []\n",
    "    likes_dimension = []\n",
    "    friends_dimension = []\n",
    "    for item in centroid4_list:\n",
    "        post_dimension.append(int(observations[item][0]))\n",
    "        comment_dimension.append(int(observations[item][1]))\n",
    "        likes_dimension.append(int(observations[item][2]))\n",
    "        friends_dimension.append(int(observations[item][3]))\n",
    "    l= []\n",
    "    if(len(post_dimension)!=0):\n",
    "        l.append(np.mean(post_dimension))\n",
    "        l.append(np.mean(comment_dimension))\n",
    "        l.append(np.mean(likes_dimension))\n",
    "        l.append(np.mean(friends_dimension))\n",
    "    else:\n",
    "        l=[0.0,0.0,0.0,0.0]\n",
    "    new_centroid[3] = l\n",
    "    return new_centroid\n",
    "\n",
    "def calculate_k_means_clustering(data_set_path, k):\n",
    "    observations = read_data_set()\n",
    "    #new_centroids = create_random_centroid_values(observations)\n",
    "    #print centroids\n",
    "    new_centroids = {0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]}\n",
    "    centroids = None\n",
    "    # TODO: Compare the new centroid dictionary with the old centroid dictionary\n",
    "    i=0\n",
    "    while centroids != new_centroids :# new and old centroid dictionaries are NOT similar, do:\n",
    "        i+=1\n",
    "        #print new_centroids\n",
    "        centroids = new_centroids\n",
    "        observation_centroids = update_observation_centroids(observations, centroids, k)\n",
    "        new_centroids = update_centroid_values(observations, observation_centroids, k)\n",
    "    print centroids\n",
    "    print new_centroids\n",
    "    print i\n",
    "    \n",
    "    centroid1_list = []\n",
    "    centroid2_list = []\n",
    "    centroid3_list = []\n",
    "    centroid4_list = []\n",
    "    for item in observation_centroids:\n",
    "        if(observation_centroids[item]==0):\n",
    "            centroid1_list.append(item)\n",
    "        if(observation_centroids[item]==1):\n",
    "            centroid2_list.append(item)\n",
    "        if(observation_centroids[item]==2):\n",
    "            centroid3_list.append(item)\n",
    "        if(observation_centroids[item]==3):\n",
    "            centroid4_list.append(item)\n",
    "    print len(centroid1_list)\n",
    "    print len(centroid2_list)\n",
    "    print len(centroid3_list)\n",
    "    print len(centroid4_list)\n",
    "        \n",
    "        \n",
    "# run code\n",
    "calculate_k_means_clustering('networkinput.csv', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity.\n",
    "In this experiment, we see that people with same number of posts, people who like to comment, people with more likes and people with friends are grouped spearately. K-means algoritham clusters groups and we can find a behaviourial pattern in those groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Problem 4.2. Girvan-Newman Algorithm \n",
    "The Girvan-Newman algorithm is an efficient algorithm for computing graph clustering.\n",
    "\n",
    "Write a Python program that computes the best clustering for the Krackhardt Kite graph.\n",
    "Remember from the lecture that the central idea of the Girvan-Newman algorithm is to compute the edge betweenness in the input graph. Large betweenness value is an indication that the corresponding edge is a bridge between two clusters in the graph, and cutting that edge means isolating those clusters.\n",
    "The algorithm proceeds by determining edge betweenness values for all the edges in the graph, and removing the edge with the highest betweenness value and repeating until there are no more edges.\n",
    "The output of the Girvan-Newman algorithm is a dendrogram of clusters where individual vertices are at the bottom. Therefore, it's necessary to cut that dendrogram and determine the best cluster. Best clustering is the one with the highest graph modularity value, which is determined by the formula:\n",
    "$$ Q = \\sum_{i} (e_{ii} - a_i^2) $$\n",
    "where $e_{ii}$ sums the fraction of graph edges that connects nodes in the ith cluster. And $a_i$ is the fraction of edges that connect to the ith cluster (see lecture for details)\n",
    "The input to the Python program will be the Krackhardt Kite Graph as an igraph Graph object. The output should be a tuple of two arguments:\n",
    "The first should be the value of the best modularity corresponding to the best graph clustering (a floating point number). The second argument should be a tuple of igraph Graph objects representing the clusters. \n",
    "HINT: The edge betweenness calculation is a variation of the Ulrik Brandes algorithm for calculating betweenness centrality (exercise 1). The attached paper explains how to modify the algorithm to calculate edge betweenness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import igraph\n",
    "import Queue\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_node_degrees(g):\n",
    "    # HINT: You will need to calculate the adjacency matrix of the graph\n",
    "    # Some Graph methods from igraph could be helpful\n",
    "\n",
    "def calculate_modularity(g, original_deg_dict, m):\n",
    "    modularity = 0\n",
    "    degree_dict = calculate_node_degrees(g)\n",
    "    connected_components = g.components()\n",
    "    for i in range(len(connected_components)):\n",
    "        subgraph = connected_components.subgraph(i) # each subgraph represents a cluster in the graph\n",
    "        e = 0 # Fraction of edges that connect to cluster\n",
    "        a = 0 # Fraction of edges that connect to cluster with random connections between edges\n",
    "        for v in subgraph.vs:\n",
    "            e += degree_dict[v.index]\n",
    "            a += original_deg_dict[v.index]\n",
    "        # TODO: Calculate the modularity\n",
    "    return modularity\n",
    "        \n",
    "# TODO: Calculate edge betweeness  \n",
    "def calculate_edge_betweenness(g):\n",
    "\n",
    "def calculate_girvan_newman_clustering(g):\n",
    "    m = # TODO: The original number of edges\n",
    "    original_degree = calculate_node_degrees(g)\n",
    "    while # TODO: Graph still has edges:\n",
    "        Q = calculate_modularity(g, original_degree, m)\n",
    "        if (Q > largest_modularity):\n",
    "            largest_modulatirty = Q\n",
    "            edge_betweenness = calculate_edge_betweenness(g) # TODO: get edge with the highest betweenness value\n",
    "            # TODO: delete the edge with the maximum betweenness value\n",
    "            # Hint: Some built-in methods from igraph could be helpful\n",
    "    \n",
    "    # Finally\n",
    "    return final_graph_clustering, largest_modularity\n",
    "\n",
    "# Program's entry point:\n",
    "g = igraph.Graph.Famous('Krackhardt_Kite') # Connected, Unweighted, undirected social network\n",
    "calculate_girvan_newman_clustering(g)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
